# negative sampling 실습(11:30-)
# k=2로 설정
# sogmoid/binary_crossentropy 사용
# 실무에서 많이 쓰임

from tensorflow.keras.layers import Input, Dense, Dropout, Embedding
from tensorflow.keras.layers import Flatten, Dot, Activation
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np
from sklearn.model_selection import train_test_split

import pandas as pd
import nltk
nltk.download('punkt')
nltk.download('gutenberg')
nltk.download('stopwords')  # 불용어 목록

stopwords = nltk.corpus.stopwords.words('english')  # 등록된 stop word
# stopwords.extend([',','.','[',']',';'])

text_id = nltk.corpus.gutenberg.fileids()
text_id = text_id[:10]
text_id

from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()

text = nltk.corpus.gutenberg.raw(text_id)
text

sent = []
sent = nltk.sent_tokenize(text)
sent[:10]

words = nltk.word_tokenize(text)
words

# stemming

for idx, i in enumerate(words):
  if i == 'working' or i == 'worked':
    words[idx] = stemmer.stem(i)

stop = nltk.corpus.stopwords.words()
all_tokens = []
for word in words:
  all_tokens.append([w for w in word if w.lower() not in stop])
all_tokens

from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()

tokenizer.fit_on_texts(words)

word2idx = {}

n_idx = 0
for word in words:
  if word.lower() not in word2idx:
    word2idx[word.lower()] = n_idx
    n_idx +=1
idx2word = {v:k for k,v in word2idx.items()}
len(word2idx)

trigram = [(a.lower(),b.lower(),c.lower()) for a,b,c in nltk.trigrams(words)]
trigram

trigrams = [(word2idx[a], word2idx[b], word2idx[c]) for a,b,c in trigram]
trigrams[:10]

# trigrams.reshape(-1,1)

import pandas as pd

bigram = pd.DataFrame(trigrams)
bigram

data = bigram.iloc[:][0]
target = bigram.iloc[:][1]
data = data.reset_index(drop=True)

data = data.to_list()
target = target.to_list()

data = np.array(data)
target = np.array(target)
data

len(data)

data = data.reshape(-1,1)
target = target.reshape(-1,1)
data

target.shape

# # word2idx 생성 만약 딕셔너리에 단어가 없다면

# if word2idx.get(stem_word) == None:
#     word2idx[stem_word] = cnt
#     cnt+=1
#     sent.append(clean_tok)
    
#     print('{}: {} ----- processed.'.format(i+1, text_id))

# idx2word = {v:k for k, v in word2idx.items()}

# print("총 문장 개수 =", len(sent))
# print(sent[0])

# train_x =[]
# train_y = []

# for sentence in sent:
#   tri_grams = nltk.trigrams(sentence)
#   try:
#     for left, center, right in tri_grams:
#       train_x.append(word2idx[center])
#       train_y.append(word2idx[left])
#       train_x.append(word2idx[center])
#       train_y.append(word2idx[right])
#   except:
#     continue

# len(train_x)

# len(train_y)

# len(word2idx)

# 단어간 유사도 측정
# 모델 만들기
# 행렬로 만드는 이유. pd.ndarray()
# 불용어 제거 안하는 이유_문장의 의미가 사라짐. -> stemming만(동사원형). 토큰화
# word2idx_idx2word -> 단어 사전 만들기

from tensorflow.keras.layers import Input, Dense, Dropout, Embedding
from tensorflow.keras.layers import Flatten, Dot, Activation
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np
from sklearn.model_selection import train_test_split

# train_x = np.array(train_x)
# train_y = np.array(train_y)

# train_x.shape
# train_y.shape

# # x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2)

import numpy as np
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.stem import LancasterStemmer
from tensorflow.keras.layers import Input, Embedding, Dense
from tensorflow.keras.models import Model
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('punkt')
nltk.download('gutenberg')

# 영문 소설 10개만 사용한다.
n = 10
stemmer = LancasterStemmer()
sent_stem = []
for i, text_id in enumerate(nltk.corpus.gutenberg.fileids()[:n]):
    text = nltk.corpus.gutenberg.raw(text_id)
    sentences = nltk.sent_tokenize(text)

    # 각 단어에 Lancaster stemmer를 적용한다.
    for sentence in sentences:
        word_tok = nltk.word_tokenize(sentence)
        stem = [stemmer.stem(word) for word in word_tok]
        sent_stem.append(stem)
    print('{}: {} ----- processed.'.format(i+1, text_id))

print("총 문장 개수 =", len(sent_stem))
print(sent_stem[0])

# 단어사전
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sent_stem)

# 단어사전
word2idx = tokenizer.word_index
idx2word = {v:k for k, v in word2idx.items()}

print("사전 크기 =", len(word2idx))

# rand, randint

# 문장을 단어의 인덱스로 표현
# 왜 인덱스로 표현?

# sent_idx = tokenizer.texts_to_sequences(sent_stem)
# sent_idx[0]

# np.array(sent_idx[1])

# # trigram
# x_train = []
# y_train = []
# for sent in sent_idx:
#     if len(sent) < 3:
#         continue

#     for a, b, c in nltk.trigrams(sent):
#         x_train.append(b)
#         x_train.append(b)
#         y_train.append(a)
#         y_train.append(c)

# x_train = np.array(x_train).reshape(-1, 1)
# y_train = np.array(y_train).reshape(-1, 1)

VOC_SIZE = len(word2idx) + 1
EMB_SIZE = 32

x_input = Input(batch_shape=(None, 1))
x_emb = Embedding(VOC_SIZE, EMB_SIZE)(x_input)
y_output = Dense(VOC_SIZE, activation='sigmoid')(x_emb)

model = Model(x_input, y_output)
model.compile(loss = 'binary_crossentropy', optimizer='adam')
model.summary()

"""데이터셋 구성"""

# random 함수 추출

# random.random()

# b a              x  y1 y2
# b c  =>(같음)    b  a  c

# embed_size = 100



# 중심 단어를 위한 임베딩 테이블
w_inputs = Input(shape=(1, ), dtype='int32')
word_embedding = Embedding(vocab_size, embed_size)(w_inputs)

# 주변 단어를 위한 임베딩 테이블
c_inputs = Input(shape=(1, ), dtype='int32')
context_embedding  = Embedding(vocab_size, embed_size)(c_inputs)

dot_product = Dot(axes=2)([word_embedding, context_embedding])
dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)
output = Activation('sigmoid')(dot_product)

model = Model(inputs=[w_inputs, c_inputs], outputs=output)
model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam')
plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')

for epoch in range(1, 6):
    loss = 0
    for _, elem in enumerate(skip_grams):
        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')
        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')
        labels = np.array(elem[1], dtype='int32')
        X = [first_elem, second_elem]
        Y = labels
        loss += model.train_on_batch(X,Y)  
    print('Epoch :',epoch, 'Loss :',loss)

# word --> word2vec을 확인하기 위한 모델
model_vec = Model(x_input, x_emb)
hist = model.fit(x_train, y_train, batch_size=10000, epochs=10)

def get_word2vec(word):
    stem_word = stemmer.stem(word)
    if stem_word not in word2idx:
        print('{}가 없습니다.'.format(word))
        return
    
    word2vec = model_vec.predict(np.array(word2idx[stem_word]).reshape(1,1))[0]
    return word2vec

father = get_word2vec('father')
mother = get_word2vec('mother')
doctor = get_word2vec('doctor')

cosine_similarity(father, mother)

cosine_similarity(father, doctor)

father = model.predict(np.array(word2idx['father']).reshape(-1,1))
mother = model.predict(np.array(word2idx['mother']).reshape(-1,1))
doctor = model.predict(np.array(word2idx['doctor']).reshape(-1,1))

# 코사인 유사도

# from sklearn.metrics.pairwise import cosine_similarity

# d = cosine_similarity(father, mother)
# f = cosine_similarity(father, doctor)
# d = cosine_similarity(doctor, mother)

# 학습
# hist = model.fit(train_x, train_y, batch_size=4096, epochs = 30)

import matplotlib.pyplot as plt

plt.plot(hist.history['loss'], label='train')
plt.legend()
plt.show()

train_x =[]
train_y = []
for sentence in sent:
  tri_grams = nltk.trigrams(sentence)
  try:
    for left, center, right in tri_grams:
      train_x.append(word2idx[center])
      train_y.append(word2idx[left])
      train_x.append(word2idx[center])
      train_y.append(word2idx[right])
  except:
    continue

len(train_x)

len(train_y)

len(word2idx)

from tensorflow.keras.layers import Input, Dense, Dropout, Embedding
from tensorflow.keras.layers import Flatten, Dot, Activation
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np
from sklearn.model_selection import train_test_split

train_x = np.array(train_x)
train_y = np.array(train_y)

train_x.shape
train_y.shape

#x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2)

n_factors = 32

x_input = Input(batch_shape = (None, 1))
x_emb = Embedding(input_dim = len(word2idx)+1, output_dim = n_factors)(x_input)
y_output = Dense(len(word2idx)+1, activation='sigmoid')(x_emb)

model = Model(x_input, y_output)
model.compile(loss='binary_crossentropy', optimizer = Adam(learning_rate=0.01), metrics=['sparse_categorical_accuracy'])
model.summary()

train_x.max()

# sgns 구현

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, Reshape, Activation, Input
from tensorflow.keras.layers import Dot
from tensorflow.keras.utils import plot_model
from IPython.display import SVG

# 전처리(불필요한 토큰을 제거하고, 소문자화를 통해 정규화를 진행합니다)

news_df = pd.DataFrame({'document':documents})
# 특수 문자 제거
news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z]", " ")
# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
# 전체 단어에 대한 소문자 변환
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())

# 현재 데이터프레임에 Null 값이 있는지 확인합니다.
news_df.isnull().values.any()

#빈 값(empy) 유무도 확인해야 합니다. 모든 빈 값을 Null 값으로 변환하고, 다시 Null 값이 있는지 확인합니다.
news_df.replace("", float("NaN"), inplace=True)
news_df.isnull().values.any()

news_df.dropna(inplace=True)
print('총 샘플 수 :',len(news_df))

# 불용어를 제거
stop_words = stopwords.words('english')
tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())
tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])
tokenized_doc = tokenized_doc.to_list()

# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.
drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]
tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)
print('총 샘플 수 :',len(tokenized_doc))

# 정수 인코딩
tokenizer = Tokenizer()
tokenizer.fit_on_texts(tokenized_doc)

word2idx = tokenizer.word_index
idx2word = {v:k for k, v in word2idx.items()}
encoded = tokenizer.texts_to_sequences(tokenized_doc)

# 상위 2개의 샘플을 출력
print(encoded[:2])

# 단어 집합의 크기를 확인
vocab_size = len(word2idx) + 1 
print('단어 집합의 크기 :', vocab_size)

# 데이터셋 구성_네거티브 셈플링

from tensorflow.keras.preprocessing.sequence import skipgrams
# 네거티브 샘플링
skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]
# 10개의 뉴스그룹 샘플에 대해서 모두 수행

# 윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블을 갖도록 하고, 
# 그렇지 않은 경우는 0의 레이블을 가지도록 하여 데이터셋을 구성합니다. 

print('전체 샘플 수 :',len(skip_grams))

# 그리고 10개의 뉴스그룹 샘플 각각은 수많은 중심 단어, 주변 단어의 쌍으로 된 샘플들을 갖고 있습니다. 
# 첫번째 뉴스그룹 샘플이 가지고 있는 pairs와 labels의 개수를 출력해봅시다.

# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수
print(len(pairs))
print(len(labels))

# 이 작업을 모든 뉴스그룹 샘플에 대해서 수행

skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, Reshape, Activation, Input
from tensorflow.keras.layers import Dot
from tensorflow.keras.utils import plot_model
from IPython.display import SVG

# 정수 인코딩?
# 토큰화 하는 이유. 모델 학습 도중에 많이 하는 이유
