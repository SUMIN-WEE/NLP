{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0709_popcorn(LSTM) 실습.ipynb",
      "provenance": [],
      "mount_file_id": "1gAIt1lmYypnR8n-KxH8sRT4Qe5ZrY5W6",
      "authorship_tag": "ABX9TyMFaUYugsxo/4pqioNUyESD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUMIN-WEE/NLP/blob/master/0709_popcorn(LSTM)_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qshsIc9wPZ_m"
      },
      "source": [
        "# popcorn data를 LSTM으로 감성분석\n",
        "# (+loss, acc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0pkcPnhPaSU"
      },
      "source": [
        "# 교재 코드 lstm 분석\n",
        "\n",
        "# 교재 문장(161p)_vocabulary 만들기(keras_tokenizer) (O)\n",
        "# vocab의 index로 표현(text to sequence. 인덱스 벡터)\n",
        "# review[0] -> [404,70...] => one-hot (리뷰 하나가 2D로 변경)\n",
        "## 트리 구조 : 단어 개수 한 리뷰당 (174, vocab_size) 의 트리 구조\n",
        "\n",
        "# 길이가 다 다르기 때문에 길이를 맞춤(keras_padding ->174개로)\n",
        "## lstm_many to one : 2차원 -> 1차원 벡터로 나옴 -> ffn(받기. 2d 라 받을 수 있음)\n",
        "\n",
        "\n",
        "## 리뷰 1개_ \n",
        "## embedding layer 에 집어넣음 -> 각각의 리뷰들을 one-hot으로 변경 \n",
        "## -> 32개의 embedding size에 넣어줌 -> output : 32개의 벡터 나옴\n",
        "# 길이가 다 다르기 때문에 길이를 맞춤(keras_padding)\n",
        "## lstm, cnn_ 3차원 데이터 사용 가능\n",
        "## lstm_many to one : 2차원 -> 1차원 벡터로 나옴 -> ffn(받기. 2d 라 받을 수 있음)\n",
        "# '어떤 길이 사용?' -> eda 결과 참조, 174개로 padding (맞춰줌)\n",
        "\n",
        "# 교재 -172p부분 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtixsmaJPkaz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# 학습 데이터를 읽어온다.\n",
        "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/popcorn_data/labeledTrainData.tsv', header=0, sep='\\t', quoting=3)\n",
        "df['review'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHEEdD51PmRH"
      },
      "source": [
        "df['sentiment'].head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPTYDvd0Pmu4"
      },
      "source": [
        "# Pre-processing\n",
        "stemmer = LancasterStemmer()\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "processed_text = []\n",
        "for review in df['review']:\n",
        "    # 1. 영문자와 숫자만 사용한다. 그 이외의 문자는 공백 문자로 대체한다.\n",
        "    review = review.replace('<br />', ' ')   # <br> --> space\n",
        "    review = re.sub(\"[^a-zA-Z]\", \" \", review)    # 영문자만 사용\n",
        "\n",
        "    tmp = []\n",
        "    for word in nltk.word_tokenize(review):\n",
        "        # 2. 길이가 2 이하인 단어와 stopword는 제거한다.\n",
        "        if len(word) > 2 and word not in stopwords:\n",
        "            # 3. Lemmatize\n",
        "            tmp.append(stemmer.stem(word.lower()))\n",
        "    processed_text.append(' '.join(tmp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpAhTXahPosk"
      },
      "source": [
        "# Pre-processing\n",
        "stemmer = LancasterStemmer()\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "processed_text = []\n",
        "for review in df['review']:\n",
        "    # 1. 영문자와 숫자만 사용한다. 그 이외의 문자는 공백 문자로 대체한다.\n",
        "    review = review.replace('<br />', ' ')   # <br> --> space\n",
        "    review = re.sub(\"[^a-zA-Z]\", \" \", review)    # 영문자만 사용\n",
        "\n",
        "    tmp = []\n",
        "    for word in nltk.word_tokenize(review):\n",
        "        # 2. 길이가 2 이하인 단어와 stopword는 제거한다.\n",
        "        if len(word) > 2 and word not in stopwords:\n",
        "            # 3. Lemmatize\n",
        "            tmp.append(stemmer.stem(word.lower()))\n",
        "    processed_text.append(' '.join(tmp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E8YCYg6PqkO"
      },
      "source": [
        "# 학습 데이터\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_text, list(df['sentiment']), test_size = 0.3)\n",
        "\n",
        "# 학습 데이터를 저장해 둔다.\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/popcorn_data/popcorn.pkl', 'wb') as f:\n",
        "    pickle.dump([X_train, y_train, X_test, y_test], f, pickle.DEFAULT_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi8wNhLgPuC-"
      },
      "source": [
        "# 학습 데이터\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_text, list(df['sentiment']), test_size = 0.3)\n",
        "\n",
        "# 학습 데이터를 저장해 둔다.\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/popcorn_data/popcorn.pkl', 'wb') as f:\n",
        "    pickle.dump([X_train, y_train, X_test, y_test], f, pickle.DEFAULT_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxvL6x5_Pvad"
      },
      "source": [
        "fig,axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(6,3)\n",
        "sns.countplot(df['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9sby5cVP15h"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import pickle\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "# 학습 데이터를 읽어온다.\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/popcorn_data/popcorn.pkl', 'rb') as f:\n",
        "    X_train, y_train, X_test, y_test = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_x-SPuCP2bU"
      },
      "source": [
        "# 학습\n",
        "pipe = Pipeline([('tf_vect', TfidfVectorizer(max_df=50, max_features=10000)),\n",
        "                  ('lr_clf', LogisticRegression(max_iter = 500, C = 10))])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "pred = pipe.predict(X_test)\n",
        "pred_probs = pipe.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print('정확도 =', accuracy_score(y_test, pred), ' ROC-AUC =', roc_auc_score(y_test, pred_probs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOETyV6AP6s1"
      },
      "source": [
        "len(pipe['tf_vect'].vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KZRSyiNP9be"
      },
      "source": [
        "# padding\n",
        "\n",
        "from bs4 import BeautifulSoup \n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# def preprocessing(review, remove_stopwords = False):\n",
        "#   # html 태그 제거\n",
        "\n",
        "review = df['review'][0]\n",
        "review_text = BeautifulSoup(review, \"html5lib\").get_text()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "review_text = review_text.lower()\n",
        "words = review_text.split()\n",
        "words = [w for w in words if not w in stop_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U9CoQIgP_Y-"
      },
      "source": [
        "clean_review = ' '.join(words)\n",
        "print(clean_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu4lJtzlP_1_"
      },
      "source": [
        "def preprocessing(review, remove_stopwords = False):\n",
        "  # 대문자->소문자\n",
        "  words = review_text.split()\n",
        "\n",
        "  if remove_stopwords:\n",
        "    # 불용어 제거\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    clean_review = ' '.join(words)\n",
        "\n",
        "  else:\n",
        "    clean_review = ''.join(words)\n",
        "\n",
        "  return(clean_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kks-SXhaQB5l"
      },
      "source": [
        "clean_train_reviews = []\n",
        "for review in df['review']:\n",
        "  clean_train_reviews.append(preprocessing(review, remove_stopwords = True))\n",
        "\n",
        "len(clean_train_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj2MS6RrQEJt"
      },
      "source": [
        "clean_train_df = pd.DataFrame({'review' : clean_train_reviews, 'sentiment': df['sentiment']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfCiFc_8QGlP"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_reviews)\n",
        "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLxqZei-QHFT"
      },
      "source": [
        "word_vocab = tokenizer.word_index\n",
        "word_vocab[\"<PAD>\"]=0\n",
        "print(word_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQF7yvWnQKrg"
      },
      "source": [
        "print(len(word_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBBmG3zOQMOq"
      },
      "source": [
        "pipe = Pipeline([('tf_vect', TfidfVectorizer(max_df=50, max_features=10000)),\n",
        "                  ('lr_clf', LogisticRegression(max_iter = 500, C = 10))])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "pred = pipe.predict(X_test)\n",
        "pred_probs = pipe.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAQl6_rDQOYk"
      },
      "source": [
        "# LSTM (+ embedded layer)_이후로는 코드 진행 못함"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf_rtfKOQWae"
      },
      "source": [
        "embedding_vector_length = 32\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=top_words, # 5000\n",
        "              output_dim=embedding_vector_length, # 32\n",
        "              input_length=max_review_length), \n",
        "    LSTM(50), # 원래는 100, \n",
        "    Dropout(0.5), \n",
        "    Dense(25, activation='sigmoid'), \n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
        "print(\"training complete\")\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: {:.2f}\".format(scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaEJLi_QRC0k"
      },
      "source": [
        "# 답"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XApl0osCQ4J_"
      },
      "source": [
        "# Word Embedding & LSTM을 이용한 감성분석\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %cd '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "# 학습, 시험 데이터를 읽어온다.\n",
        "with open('data/popcorn.pkl', 'rb') as f:\n",
        "    xd_train, yd_train, xd_test, yd_test = pickle.load(f)\n",
        "\n",
        "xd_train[0]\n",
        "\n",
        "# 학습, 시험 데이터를 합쳐서 사전 (vocabulary)을 구축한다.\n",
        "x_data = xd_train + xd_test\n",
        "y_target = np.array(yd_train + yd_test).reshape(-1,1)\n",
        "\n",
        "# 사전을 구축하고 학습 데이터를 워드 인덱스로 표현한다.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_data)\n",
        "x_idx = tokenizer.texts_to_sequences(x_data)\n",
        "\n",
        "# vocabulary\n",
        "word2idx = tokenizer.word_index\n",
        "word2idx['<PAD>'] = 0\n",
        "\n",
        "# 각 리뷰의 길이를 측정하고, 평균 길이를 구한다.\n",
        "x_len = [len(x) for x in x_idx]\n",
        "\n",
        "print('평균 =', np.mean(x_len))\n",
        "print('최소 =', np.min(x_len))\n",
        "print('최대 =', np.max(x_len))\n",
        "print('중앙값 =', np.median(x_len))\n",
        "\n",
        "# 길이 분포를 확인한다.\n",
        "plt.hist(x_len, bins=50)\n",
        "plt.show()\n",
        "\n",
        "# 각 리뷰의 길이를 max_seq_len로 맞춘다. 길면 자르고, 짧으면 padding을 추가한다.\n",
        "MAX_SEQ_LEN = 200\n",
        "x_review = pad_sequences(x_idx, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "x_review[5000]\n",
        "\n",
        "# 학습 데이터와 시험 데이터로 분리한다.\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_review, y_target, test_size=0.2)\n",
        "\n",
        "# Embedding & LSTM 모델을 생성한다.\n",
        "vocab_size = len(word2idx)\n",
        "EMBEDDING_DIM = 32\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "x_input = Input(batch_shape=(None, x_train.shape[1]))\n",
        "e_layer = Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM)(x_input)\n",
        "e_layer = Dropout(rate=0.5)(e_layer)\n",
        "r_layer = LSTM(HIDDEN_DIM, dropout=0.5)(e_layer)\n",
        "y_output = Dense(1, activation='sigmoid')(r_layer)\n",
        "\n",
        "model = Model(x_input, y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0005))\n",
        "model.summary()\n",
        "\n",
        "# 학습\n",
        "hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size = 1024, epochs = 30)\n",
        "\n",
        "# Loss history를 그린다\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 시험 데이터로 학습 성능을 평가한다\n",
        "pred = model.predict(x_test)\n",
        "y_pred = np.where(pred > 0.5, 1, 0)\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(\"\\nAccuracy = %.2f %s\" % (accuracy * 100, '%'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}